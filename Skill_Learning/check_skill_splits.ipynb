{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04d32ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skill_helpers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "805be769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import joblib \n",
    "import json \n",
    "from joblib import load as joblib_load\n",
    "\n",
    "\n",
    "# --- must match your training definitions ---\n",
    "class ImageNormalizer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean, dtype=torch.float32).view(3,1,1)\n",
    "        self.std  = torch.clamp(torch.tensor(std, dtype=torch.float32).view(3,1,1), min=1e-3)\n",
    "    def __call__(self, x):  # x: [3,H,W] in [0,1]\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(c_in, c_out, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "        self.bn   = torch.nn.BatchNorm2d(c_out)  # or GroupNorm if you switched\n",
    "        self.act  = torch.nn.GELU()\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class PolicyCNN(torch.nn.Module):\n",
    "    def __init__(self, n_actions=16):\n",
    "        super().__init__()\n",
    "        self.stem = torch.nn.Sequential(\n",
    "            ConvBlock(3, 32, k=7, s=2, p=3),\n",
    "            ConvBlock(32, 32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.stage2 = torch.nn.Sequential(\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 64),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.stage3 = torch.nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.stage4 = torch.nn.Sequential(\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 256),\n",
    "        )\n",
    "        self.head = torch.nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.head(x)\n",
    "\n",
    "# ---- inference helpers ----\n",
    "\n",
    "def load_policy(ckpt_path, device=None):\n",
    "    \"\"\"Load model + normalizer from a saved training checkpoint.\"\"\"\n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    n_actions = int(ckpt['n_actions'])\n",
    "    model = PolicyCNN(n_actions=n_actions).to(device)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.eval()\n",
    "    normalizer = ImageNormalizer(ckpt['mean'], ckpt['std'])\n",
    "    return model, normalizer, device, n_actions\n",
    "\n",
    "def preprocess_frame(frame_hw3, normalizer, target=256):\n",
    "    \"\"\"\n",
    "    frame_hw3: numpy array [H,W,3], float32 in [0,1]\n",
    "    returns torch tensor [1,3,target,target]\n",
    "    \"\"\"\n",
    "    assert frame_hw3.ndim == 3 and frame_hw3.shape[2] == 3\n",
    "    x = torch.from_numpy(np.transpose(frame_hw3, (2,0,1))).float()   # [3,H,W]\n",
    "    x = F.interpolate(x.unsqueeze(0), size=(target, target), mode='bilinear', align_corners=False).squeeze(0)  # [3,T,T]\n",
    "    x = normalizer(x)\n",
    "    return x.unsqueeze(0)  # [1,3,T,T]\n",
    "\n",
    "@torch.no_grad()\n",
    "def act_greedy(model, normalizer, device, frame_hw3):\n",
    "    \"\"\"\n",
    "    Returns (action_id, probs) where probs is a numpy array length n_actions.\n",
    "    \"\"\"\n",
    "    x = preprocess_frame(frame_hw3, normalizer)            # [1,3,256,256]\n",
    "    x = x.to(device)\n",
    "    logits = model(x)                                      # [1,n_actions]\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze(0)       # [n_actions]\n",
    "    action = int(torch.argmax(probs).item())\n",
    "    return action, probs.cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def act_sample(model, normalizer, device, frame_hw3, temperature=1.0):\n",
    "    x = preprocess_frame(frame_hw3, normalizer).to(device)\n",
    "    logits = model(x).squeeze(0)\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / max(1e-6, float(temperature))\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    action = int(torch.multinomial(probs, num_samples=1).item())\n",
    "    return action, probs.cpu().numpy()\n",
    "\n",
    "def load_pu_start_models(models_dir: str):\n",
    "    \"\"\"\n",
    "    Load (skill, clf, threshold, meta) tuples from <models_dir>.\n",
    "    Expects files: <skill>_clf.joblib and <skill>_meta.json\n",
    "    Returns: list[dict] with keys: skill, clf, thr, meta\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for fname in os.listdir(models_dir):\n",
    "        if not fname.endswith(\"_meta.json\"):\n",
    "            continue\n",
    "        skill = fname[:-10]  # strip \"_meta.json\"\n",
    "        meta_path  = os.path.join(models_dir, f\"{skill}_meta.json\")\n",
    "        model_path = os.path.join(models_dir, f\"{skill}_clf.joblib\")\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "        try:\n",
    "            with open(meta_path, \"r\") as f:\n",
    "                meta = json.load(f)\n",
    "            thr = float(meta[\"threshold\"])\n",
    "            clf = joblib_load(model_path)\n",
    "            models.append({\"skill\": skill, \"clf\": clf, \"thr\": thr, \"meta\": meta})\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {skill}: {e}\")\n",
    "    return models\n",
    "\n",
    "def applicable_pu_start_models(models, state, *, return_details=False, eps=0.0):\n",
    "    \"\"\"\n",
    "    Given a list from load_pu_models(...) and a state feature vector (shape [d] or [1,d]),\n",
    "    return/print skills whose probability >= threshold (+eps).\n",
    "    - return_details=True returns a list of dicts with scores/margins\n",
    "    - eps lets you demand a small margin above threshold (e.g., eps=0.02).\n",
    "    \"\"\"\n",
    "    # Accept 1D or 2D input\n",
    "    state = np.asarray(state)\n",
    "    if state.ndim == 1:\n",
    "        X = state.reshape(1, -1)\n",
    "    elif state.ndim == 2 and state.shape[0] == 1:\n",
    "        X = state\n",
    "    else:\n",
    "        raise ValueError(\"`state` must be a single feature vector of shape [d] or [1,d].\")\n",
    "\n",
    "    rows = []\n",
    "    for m in models:\n",
    "        prob = float(m[\"clf\"].predict_proba(X)[:, 1][0])\n",
    "        thr  = float(m[\"thr\"])\n",
    "        margin = prob - thr\n",
    "        is_applicable = prob >= (thr + eps)\n",
    "        rows.append({\n",
    "            \"skill\": m[\"skill\"],\n",
    "            \"prob\": prob,\n",
    "            \"thr\": thr,\n",
    "            \"margin\": margin,\n",
    "            \"applicable\": is_applicable\n",
    "        })\n",
    "\n",
    "    # Sort by confidence margin (best first)\n",
    "    rows.sort(key=lambda r: r[\"margin\"], reverse=True)\n",
    "\n",
    "    # Print list of applicable models\n",
    "    applicable = [r for r in rows if r[\"applicable\"]]\n",
    "    # if applicable:\n",
    "    #     print(\"Applicable models (prob ≥ threshold):\")\n",
    "    #     for r in applicable:\n",
    "    #         print(f\"  - {r['skill']}: p={r['prob']:.3f}  thr={r['thr']:.3f}  margin={r['margin']:.3f}\")\n",
    "    # else:\n",
    "    #     print(\"No applicable models for this state.\")\n",
    "\n",
    "    return rows if return_details else [r[\"skill\"] for r in applicable]\n",
    "\n",
    "def load_pu_end_model(models_dir: str, skill: str):\n",
    "    \"\"\"\n",
    "    Load a single PU model (classifier + metadata) for a given skill.\n",
    "\n",
    "    Looks for:\n",
    "      - <models_dir>/<skill>_clf.joblib\n",
    "      - <models_dir>/<skill>_meta.json\n",
    "\n",
    "    Returns:\n",
    "      dict with keys:\n",
    "        - skill: str\n",
    "        - clf:   fitted classifier (expects .predict_proba)\n",
    "        - thr:   float threshold from meta[\"threshold\"]\n",
    "        - meta:  dict (entire meta JSON)\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(models_dir, f\"{skill}_clf.joblib\")\n",
    "    meta_path  = os.path.join(models_dir, f\"{skill}_meta.json\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Missing model file: {model_path}\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Missing meta file:  {meta_path}\")\n",
    "\n",
    "    clf = joblib_load(model_path)\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    thr = float(meta[\"threshold\"])\n",
    "    return {\"skill\": skill, \"clf\": clf, \"thr\": thr, \"meta\": meta}\n",
    "\n",
    "\n",
    "def predict_pu_end_state(model: dict, state) -> dict:\n",
    "    \"\"\"\n",
    "    Score a single state with a loaded PU model dict from load_pu_model(...).\n",
    "\n",
    "    Args:\n",
    "      - model: dict with keys {\"skill\",\"clf\",\"thr\",\"meta\"}\n",
    "      - state: shape [d] or [1, d]\n",
    "\n",
    "    Returns:\n",
    "      dict: {prob, threshold, is_end, margin}\n",
    "    \"\"\"\n",
    "    # Accept 1D or 2D single-row input\n",
    "    state = np.asarray(state)\n",
    "    if state.ndim == 1:\n",
    "        X = state.reshape(1, -1)\n",
    "    elif state.ndim == 2 and state.shape[0] == 1:\n",
    "        X = state\n",
    "    else:\n",
    "        raise ValueError(\"`state` must be a single feature vector of shape [d] or [1, d].\")\n",
    "\n",
    "    # Compute positive-class probability\n",
    "    prob = float(model[\"clf\"].predict_proba(X)[:, 1][0])\n",
    "\n",
    "    thr = float(model[\"thr\"])\n",
    "    margin = prob - thr\n",
    "    return {\n",
    "        \"prob\": prob,\n",
    "        \"threshold\": thr,\n",
    "        \"is_end\": bool(prob >= thr),\n",
    "        \"margin\": margin,\n",
    "    }\n",
    "\n",
    "def load_all_models(skill_list = ['wood', 'stone', 'wood_pickaxe', 'stone_pickaxe', 'table']):\n",
    "    bc_models = {}\n",
    "    for skill in skill_list:\n",
    "        ckpt_path = os.path.join('../../Craftax/Traces/stone_pickaxe_easy', 'bc_checkpoints', f'{skill}_policy_cnn.pt')\n",
    "        bc_models[skill] = load_policy(ckpt_path)\n",
    "\n",
    "    artifacts = joblib.load('../../Craftax/Traces/stone_pickaxe_easy/pca_models/pca_model_750.joblib')\n",
    "    scaler = artifacts['scaler']\n",
    "    pca = artifacts['pca']\n",
    "    n_features_expected = scaler.mean_.shape[0]\n",
    "\n",
    "    pu_start_models = load_pu_start_models('../../Craftax/Traces/stone_pickaxe_easy/pu_start_models')\n",
    "\n",
    "    pu_end_models = {}\n",
    "    for skill in skill_list:\n",
    "        try:\n",
    "            pu_end_models[skill] = load_pu_end_model('../../Craftax/Traces/stone_pickaxe_easy/pu_end_models', skill)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] No PU end model for skill '{skill}'\")\n",
    "\n",
    "    return {\n",
    "        \"skills\": skill_list,  # <—— canonical order\n",
    "        \"bc_models\": bc_models,\n",
    "        \"termination_models\": pu_end_models,\n",
    "        \"start_models\": pu_start_models,\n",
    "        \"pca_model\": {'scaler': scaler, 'pca': pca, 'n_features_expected': n_features_expected}\n",
    "    }\n",
    "\n",
    "def available_skills(models, state):\n",
    "    # state: uint8 or float32 flat vector -> PCA space\n",
    "    state = np.asarray(state).astype(np.float32)\n",
    "    if state.max() > 1.0:  # allow uint8 input\n",
    "        state = state / 255.0\n",
    "\n",
    "    X = state.reshape(1, -1)\n",
    "    Xc = models[\"pca_model\"]['scaler'].transform(X)\n",
    "    Xf = models[\"pca_model\"]['pca'].transform(Xc)\n",
    "\n",
    "    rows = applicable_pu_start_models(models[\"start_models\"], Xf, return_details=True, eps=0.0)\n",
    "    applicable = {r[\"skill\"] for r in rows if r[\"applicable\"]}\n",
    "    order = models[\"skills\"]\n",
    "    return np.array([s in applicable for s in order], dtype=bool)\n",
    "\n",
    "def should_terminate(models, state, skill): \n",
    "    state = np.asarray(state).astype(np.float32) / 255.0\n",
    "\n",
    "    X = state.reshape(1, -1)\n",
    "    X_centered = models[\"pca_model\"]['scaler'].transform(X)\n",
    "    X_feats = models[\"pca_model\"]['pca'].transform(X_centered)\n",
    "\n",
    "    return predict_pu_end_state(models[\"termination_models\"][skill], X_feats)[\"is_end\"]\n",
    "\n",
    "\n",
    "\n",
    "def bc_policy(models, state, skill):\n",
    "\n",
    "    assert state.max() > 1.0 \n",
    "     # allow uint8 input\n",
    "\n",
    "    state = np.asarray(state).astype(np.float32) / 255.0\n",
    "\n",
    "    model, normalizer, device, n_actions = models[\"bc_models\"][skill]\n",
    "    action, probs = act_greedy(model, normalizer, device, state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41a44371",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Craftax/Traces/stone_pickaxe_easy/groundTruth/craftax_0', 'r') as file:\n",
    "    skill_lines = file.readlines()\n",
    "test = np.load( '../../Craftax/Traces/stone_pickaxe_easy/pixel_obs/craftax_0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11283e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 wood\n",
      "1 wood\n",
      "2 wood\n",
      "3 wood\n",
      "4 wood\n",
      "5 table\n",
      "6 table\n",
      "7 wood\n",
      "8 wood\n",
      "9 wood\n",
      "10 wood\n",
      "11 wood_pickaxe\n",
      "12 wood_pickaxe\n",
      "13 wood_pickaxe\n",
      "14 stone\n",
      "15 stone\n",
      "16 stone\n",
      "17 wood\n",
      "18 wood\n",
      "19 wood\n",
      "20 wood\n",
      "21 stone_pickaxe\n",
      "22 stone_pickaxe\n",
      "23 stone_pickaxe\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(skill_lines):\n",
    "    print(i, n.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58c47267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damionharvey/miniconda3/envs/hisd/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/damionharvey/miniconda3/envs/hisd/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mods = load_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "067fbdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "should_terminate(mods, test[16], 'stone' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hisd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
